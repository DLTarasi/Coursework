library("kknn")
#set Directory to data path
setwd('/Users/dave/isye6501/Week 2')
#create table of credit card data - no headers
data<-read.table("credit_card_data.txt", header = FALSE)
#Check class and inspect data
class(data)
head(data)
summary(data)
set.seed(10) # Set seed used in sampling so that we can reproduce sample
#generate train/test split
# sample 85% of data and assign to train set. Assign remaining 15% to test set
sample <- sample.int(n = nrow(data), size = floor(.7*nrow(data)), replace = FALSE)
train <- data[sample, ]
test  <- data[-sample, ]
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks and Folds.
cv<-cv.kknn(formula=V11~.,data=train, k=27, kernel="rectangular", distance=2,scale=TRUE, kcv = 100)
cv$yhat<-round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
cv = data.table(cv[[1]])
table(cv$y == cv$yhat)
cv$yhat<-round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
cv = data.table(cv[[1]])
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
# Cross Validation Accuracy - compare actual vs. predicted
cv = data.table(cv[[1]])
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
cv
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks and Folds.
cv<-cv.kknn(formula=V11~.,data=train, k=27, kernel="rectangular", distance=2,scale=TRUE, kcv = 100)
cv
cv$yhat<-round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
cv = data.table(cv[[1]])
cv$yhat<-round(cv$yhat)
library(data.table)
library("kknn")
#set Directory to data path
setwd('/Users/dave/isye6501/Week 2')
#create table of credit card data - no headers
data<-read.table("credit_card_data.txt", header = FALSE)
#Check class and inspect data
class(data)
head(data)
summary(data)
set.seed(10) # Set seed used in sampling so that we can reproduce sample
#generate train/test split
# sample 85% of data and assign to train set. Assign remaining 15% to test set
sample <- sample.int(n = nrow(data), size = floor(.7*nrow(data)), replace = FALSE)
train <- data[sample, ]
test  <- data[-sample, ]
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks and Folds.
cv<-cv.kknn(formula=V11~.,data=train, k=27, kernel="rectangular", distance=2,scale=TRUE, kcv = 100)
cv$yhat<-round(cv$yhat)
cv<-as.data.frame(cv)
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks and Folds.
cv<-cv.kknn(formula=V11~.,data=train, k=27, kernel="rectangular", distance=2,scale=TRUE, kcv = 100)
cv<-as.data.frame(cv)
cv<-as.matrix(cv)
cv
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks and Folds.
cv<-cv.kknn(formula=V11~.,data=train, k=27, kernel="rectangular", distance=2,scale=TRUE, kcv = 100)
cv
cv[1]
cv[1]
# Cross Validation Accuracy - compare actual vs. predicted
cv = data.table(cv[[1]])
cv[1]
cv[2]
cv[,2]
cv$yhat
cv$yhat <- round(cv$yhat)
cv
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks and Folds.
cv<-cv.kknn(formula=V11~.,data=train, k=22, kernel="rectangular", distance=2,scale=TRUE, kcv = 100)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks and Folds.
cv<-cv.kknn(formula=V11~.,data=train, k=7, kernel="rectangular", distance=2,scale=TRUE, kcv = 100)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks and Folds.
cv<-cv.kknn(formula=V11~.,data=train, k=7, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=5, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=12, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=13, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=13, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=17, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=27, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=100, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=22, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#final test with best hyperparameters
model<-kknn(V11~.,train,test,k=7,distance=2,kernel="rectangular")
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=17, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=12, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=13, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=14, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=15, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#final test with best hyperparameters
model<-kknn(V11~.,train,test,k=7,distance=2,kernel="rectangular")
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=16, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=10, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#use cv.kknn to to perform k-fold cross validation. Test variety of Ks.
cv<-cv.kknn(formula=V11~.,data=train, k=13, kernel="rectangular", distance=2,scale=TRUE, kcv = 10)
cv = data.table(cv[[1]])
cv$yhat <- round(cv$yhat)
cv
# Cross Validation Accuracy - compare actual vs. predicted
table(cv$y == cv$yhat)
prop.table(table(cv$y == cv$yhat))
#final test with best hyperparameters
model<-kknn(V11~.,train,test,k=13,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(test[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(test)
accu
summary(data)
#generate train/test/val split
set.seed(10) # Set seed used in sampling so that we can reproduce sample
# sample 70% of data and assign to train set. Split remaining 30% 2/3 to validation
# and 1/3 to test set (so 20% of total to validation, 10% of total data set to test).
sample <- sample.int(n = nrow(data), size = floor(.7*nrow(data)), replace = FALSE)
sample2 <- sample.int(n = nrow(data[-sample,]), size = floor(.66*nrow(data[-sample,])), replace = FALSE)
train <- data[sample, ]
library("kknn")
#set Directory to data path
setwd('/Users/dave/isye6501/Week 2')
#create table of credit card data - no headers
data<-read.table("credit_card_data.txt", header = FALSE)
#Check class and inspect data
class(data)
head(data)
summary(data)
#generate train/test/val split
set.seed(10) # Set seed used in sampling so that we can reproduce sample
# sample 70% of data and assign to train set. Split remaining 30% 2/3 to validation
# and 1/3 to test set (so 20% of total to validation, 10% of total data set to test).
sample <- sample.int(n = nrow(data), size = floor(.7*nrow(data)), replace = FALSE)
sample2 <- sample.int(n = nrow(data[-sample,]), size = floor(.66*nrow(data[-sample,])), replace = FALSE)
train <- data[sample, ]
val <- data[sample2,]
test  <- data[-sample, ][-sample2,]
#create model - try a variety of hyperparameters using the validation set
model<-kknn(V11~.,train,val,k=2,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(val[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(val)
accu
#create model - try a variety of hyperparameters using the validation set
model<-kknn(V11~.,train,val,k=13,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(val[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(val)
accu
#final test with best hyperparameters
model<-kknn(V11~.,train,test,k=2,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(test[,11], predictions)
#create model - try a variety of hyperparameters using the validation set
model<-kknn(V11~.,train,val,k=22,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(val[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(val)
accu
#final test with best hyperparameters
model<-kknn(V11~.,train,test,k=2,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(test[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(test)
accu
#create model - try a variety of hyperparameters using the validation set
model<-kknn(V11~.,train,val,k=7,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(val[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(val)
accu
#final test with best hyperparameters
model<-kknn(V11~.,train,test,k=7,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(test[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(test)
accu
#generate train/test/val split
set.seed(111) # Set seed used in sampling so that we can reproduce sample
# sample 70% of data and assign to train set. Split remaining 30% 2/3 to validation
# and 1/3 to test set (so 20% of total to validation, 10% of total data set to test).
sample <- sample.int(n = nrow(data), size = floor(.7*nrow(data)), replace = FALSE)
sample2 <- sample.int(n = nrow(data[-sample,]), size = floor(.66*nrow(data[-sample,])), replace = FALSE)
train <- data[sample, ]
val <- data[sample2,]
test  <- data[-sample, ][-sample2,]
#create model - try a variety of hyperparameters using the validation set
model<-kknn(V11~.,train,val,k=7,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(val[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(val)
accu
#final test with best hyperparameters
model<-kknn(V11~.,train,test,k=7,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(test[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(test)
accu
# sample 70% of data and assign to train set. Split remaining 30% 1/2 to validation
# and 1/2 to test set (so 15% of total to validation, 15% of total data set to test).
sample <- sample.int(n = nrow(data), size = floor(.7*nrow(data)), replace = FALSE)
sample2 <- sample.int(n = nrow(data[-sample,]), size = floor(.5*nrow(data[-sample,])), replace = FALSE)
train <- data[sample, ]
val <- data[sample2,]
test  <- data[-sample, ][-sample2,]
#create model - try a variety of hyperparameters using the validation set
model<-kknn(V11~.,train,val,k=7,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(val[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(val)
accu
#final test with best hyperparameters
model<-kknn(V11~.,train,test,k=7,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(test[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(test)
accu
library("kknn")
#set Directory to data path
setwd('/Users/dave/isye6501/Week 2')
#create table of credit card data - no headers
data<-read.table("credit_card_data.txt", header = FALSE)
#Check class and inspect data
class(data)
head(data)
summary(data)
#generate train/test/val split
set.seed(10) # Set seed used in sampling so that we can reproduce sample
# sample 70% of data and assign to train set. Split remaining 30% 1/2 to validation
# and 1/2 to test set (so 15% of total to validation, 15% of total data set to test).
sample <- sample.int(n = nrow(data), size = floor(.7*nrow(data)), replace = FALSE)
sample2 <- sample.int(n = nrow(data[-sample,]), size = floor(.5*nrow(data[-sample,])), replace = FALSE)
train <- data[sample, ]
val <- data[sample2,]
test  <- data[-sample, ][-sample2,]
#create model - try a variety of hyperparameters using the validation set
model<-kknn(V11~.,train,val,k=7,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(val[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(val)
accu
#final test with best hyperparameters
model<-kknn(V11~.,train,test,k=7,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(test[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(test)
accu
#create model - try a variety of hyperparameters using the validation set
model<-kknn(V11~.,train,val,k=22,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(val[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(val)
accu
#final test with best hyperparameters
model<-kknn(V11~.,train,test,k=22,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(test[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(test)
accu
#create model - try a variety of hyperparameters using the validation set
model<-kknn(V11~.,train,val,k=13,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(val[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(val)
accu
#final test with best hyperparameters
model<-kknn(V11~.,train,test,k=13,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(test[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(test)
accu
#create model - try a variety of hyperparameters using the validation set
model<-kknn(V11~.,train,val,k=7,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(val[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(val)
accu
#final test with best hyperparameters
model<-kknn(V11~.,train,test,k=7,distance=2,kernel="rectangular")
#get predictions and convert from continuous reponse to binary by rounding to 0 or 1
predictions<-round(fitted(model))
predictions
# generate confusion matrix - 0,0 means model predicts 0 and actual was 0
confusion <- table(test[,11], predictions)
confusion
# Compute accuracy on val
accu <- sum(diag(confusion))/nrow(test)
accu
